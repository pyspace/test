

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Documentation of External and Wrapped Nodes &mdash; pySPACE 0.5 alpha documentation</title>
    
    <link rel="stylesheet" href="_static/pySPACE.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.5 alpha',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/pyspace-logo.ico"/>
    <link rel="top" title="pySPACE 0.5 alpha documentation" href="index.html" />
    <link rel="up" title="Table of Contents" href="content.html" />
    <link rel="prev" title="socket_utils" href="api/generated/pySPACE.tools.socket_utils.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.socket_utils.html" title="socket_utils"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">pySPACE 0.5 alpha documentation</a> &raquo;</li>
          <li><a href="content.html" accesskey="U">Table of Contents</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/pyspace-logo_small.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Documentation of External and Wrapped Nodes</a><ul>
<li><a class="reference internal" href="#scikit-nodes">Scikit Nodes</a><ul>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-binarizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ccasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-isomapsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ldasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LDASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nmfsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-normalizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-pcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-plscanonicalsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-plsregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-plssvdsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-qdasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.QDASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-rfesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFESklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-svcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-scalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode</span></tt></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="api/generated/pySPACE.tools.socket_utils.html"
                        title="previous chapter">socket_utils</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/external_nodes.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="documentation-of-external-and-wrapped-nodes">
<span id="external-nodes"></span><h1>Documentation of External and Wrapped Nodes<a class="headerlink" href="#documentation-of-external-and-wrapped-nodes" title="Permalink to this headline">¶</a></h1>
<p>pySPACE comes along with wrappers to external algorithms.</p>
<p>For details on the usage of the nodes and for getting usage examples,
have a look at their documentation.
Module for external node wrapping: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.external.html#module-pySPACE.missions.nodes.external" title="pySPACE.missions.nodes.external"><tt class="xref py py-mod docutils literal"><span class="pre">pySPACE.missions.nodes.external</span></tt></a></p>
<div class="section" id="scikit-nodes">
<span id="id1"></span><h2>Scikit Nodes<a class="headerlink" href="#scikit-nodes" title="Permalink to this headline">¶</a></h2>
<p>Nodes from <a class="reference internal" href="api/generated/pySPACE.missions.nodes.scikits_nodes.html#module-pySPACE.missions.nodes.scikits_nodes" title="pySPACE.missions.nodes.scikits_nodes"><tt class="xref py py-mod docutils literal"><span class="pre">scikits</span> <span class="pre">wrapper</span></tt></a></p>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">BernoulliNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Naive Bayes classifier for multivariate Bernoulli models.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.BernoulliNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>binarize <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd>Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size=[n_classes,]</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>class_log_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Log probability of each class (smoothed).</dd>
<dt><cite>feature_log_prob_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>Empirical log probability of features given a class, P(x_i|y).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234–265.</p>
<p>A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41–48.</p>
<p>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes &#8211; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>BernoulliNBSklearn</strong></li>
<li><strong>BernoulliNBSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-binarizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-binarizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">BinarizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Binarize data (set feature values to 0 or 1) according to a threshold</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.Binarizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The default threshold is 0.0 so that any non-zero values are set to 1.0
and zeros are left untouched.</p>
<p>Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurences for instance.</p>
<p>It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modeled using the Bernoulli
distribution in a Bayesian setting).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (0.0 by default)</span></dt>
<dd>The lower bound that triggers feature values to be replaced by 1.0.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>set to False to perform inplace binarization and avoid a copy (if
the input is already a numpy array or a scipy.sparse CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>If the input is a sparse matrix, only the non-zero values are subject
to update by the Binarizer class.</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>BinarizerSklearn</strong></li>
<li><strong>BinarizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ccasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.CCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.CCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ccasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.CCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">CCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.CCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>CCA Canonical Correlation Analysis. CCA inherits from PLS with
mode=&#8221;B&#8221; and deflation_mode=&#8221;canonical&#8221;.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pls.CCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like of predictors, shape = [n_samples, p]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
p is the number of predictors.</dd>
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">array-like of response, shape = [n_samples, q]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
q is the number of response variables.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2).</span></dt>
<dd>number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>whether to scale the data?</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop (used
only if algorithm=&#8221;nipals&#8221;)</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real, default 1e-06.</span></dt>
<dd>the tolerance used in the iterative algorithm</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether the deflation be done on a copy. Let the default value
to True unless you don&#8217;t care about side effects</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>x_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><cite>y_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><cite>x_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><cite>y_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><cite>x_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><cite>y_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><cite>x_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><cite>y_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For each component k, find the weights u, v that maximizes
max corr(Xk u, Yk v), such that <tt class="docutils literal"><span class="pre">|u|</span> <span class="pre">=</span> <span class="pre">|v|</span> <span class="pre">=</span> <span class="pre">1</span></tt></p>
<p>Note that it maximizes only the correlations between the scores.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on the
current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current Y score.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pls</span> <span class="kn">import</span> <span class="n">PLSCanonical</span><span class="p">,</span> <span class="n">PLSRegression</span><span class="p">,</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span> <span class="o">=</span> <span class="n">CCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="p">,</span> <span class="n">Y_c</span> <span class="o">=</span> <span class="n">cca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>In french but still a reference:</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<p>See also</p>
<p>PLSCanonical
PLSSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>CCASklearnNode</strong></li>
<li><strong>CCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">CountVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of text documents to a matrix of token counts</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This implementation produces a sparse representation of the counts using
scipy.sparse.coo_matrix.</p>
<p>If you do not provide an a-priori dictionary and you do not use an analyzer
that does some kind of feature selection then the number of features will
be equal to the vocabulary size found by analysing the data. The default
analyzer does simple stop word filtering for English.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span></dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>charset <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span></dt>
<dd>If bytes or files are given to analyze, this charset is used to
decode.</dd>
<dt>charset_error <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>charset</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span></dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span></dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned is currently the only
supported string value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Convert all characters to lowercase befor tokenizing.</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1.0 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly higher than the given threshold (corpus specific stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 2 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly lower than the given threshold. This value is also called
cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">optional, None by default</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, False by default.</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>vocabulary_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A mapping of terms to feature indices.</dd>
<dt><cite>stop_words_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">set</span></dt>
<dd>Terms that were ignored because
they occurred in either too many
(<cite>max_df</cite>) or in too few (<cite>min_df</cite>) documents.
This is only available if no vocabulary was given.</dd>
</dl>
<p>See also</p>
<p>HashingVectorizer, TfidfVectorizer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>CountVectorizerSklearn</strong></li>
<li><strong>CountVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DecisionTreeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A decision tree classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on</li>
</ul>
</li>
<li>classification tasks and <cite>max_features=n_features</cite></li>
<li>on regression problems.</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>tree_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object.</dd>
<dt><cite>classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
<dd>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances
(the higher, the more important the feature).
The importance of a feature is computed as the
(normalized) total reduction of error brought by that
feature.  It is also known as the Gini importance <a href="#id32"><span class="problematic" id="id33"><span id="id2"></span>[4]_</span></a>.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DecisionTreeClassifierSklearnNode</strong></li>
<li><strong>DecisionTreeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DecisionTreeRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A tree regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on</li>
</ul>
</li>
<li>classification tasks and <cite>max_features=n_features</cite></li>
<li>on regression problems.</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>tree_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances
(the higher, the more important the feature).
The importance of a feature is computed as the
(normalized) total reduction of error brought by that
feature.  It is also known as the Gini importance <a href="#id34"><span class="problematic" id="id35"><span id="id7"></span>[4]_</span></a>.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>R2 scores (a.k.a. coefficient of determination) over 10-folds CV:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,</span>
<span class="go">        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DecisionTreeRegressorSklearn</strong></li>
<li><strong>DecisionTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DictVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Transforms lists of feature-value mappings to vectors.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.dict_vectorizer.DictVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This transformer turns lists of mappings (dict-like objects) of feature
names to feature values into Numpy arrays or scipy.sparse matrices for use
with scikit-learn estimators.</p>
<p>When feature values are strings, this transformer will do a binary one-hot
(aka one-of-K) coding: one boolean-valued feature is constructed for each
of the possible string values that the feature can take on. For instance,
a feature &#8220;f&#8221; that can take on the values &#8220;ham&#8221; and &#8220;spam&#8221; will become two
features in the output, one signifying &#8220;f=ham&#8221;, the other &#8220;f=spam&#8221;.</p>
<p>Features that do not occur in a sample (mapping) will have a zero value
in the resulting array/matrix.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span></dt>
<dd>The type of feature values. Passed to Numpy array/scipy.sparse matrix
constructors as the dtype argument.</dd>
<dt>separator: string, optional</dt>
<dd>Separator string used when constructing new features for one-hot
coding.</dd>
<dt>sparse: boolean, optional.</dt>
<dd>Whether transform should produce scipy.sparse matrices.
True by default.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;bar&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">&#39;baz&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[ 2.,  0.,  1.],</span>
<span class="go">       [ 0.,  1.,  3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span>         <span class="p">[{</span><span class="s">&#39;bar&#39;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="p">{</span><span class="s">&#39;baz&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">3.0</span><span class="p">}]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">transform</span><span class="p">({</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">&#39;unseen_feature&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
<span class="go">array([[ 0.,  0.,  4.]])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DictVectorizerSklearn</strong></li>
<li><strong>DictVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DictionaryLearningSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Dictionary learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.DictionaryLearning</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-python"><pre>(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
            (U,V)
            with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components</pre>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>maximum number of iterations to perform</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>tolerance for numerical error</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd>Algorithm used to transform the data
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection <tt class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></tt></dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
<dt>code_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>initial value for the code, for warm restart</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial values for the dictionary, for warm restart</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>dictionary atoms extracted from the data</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>vector of errors at each iteration</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DictionaryLearningSklearnNode</strong></li>
<li><strong>DictionaryLearningSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extremely randomized tree classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>See also</p>
<p>ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreeClassifierSklearnNode</strong></li>
<li><strong>ExtraTreeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreeRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extremely randomized tree regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>See also</p>
<p>ExtraTreeClassifier : A classifier base on extremely randomized trees
sklearn.ensemble.ExtraTreesClassifier : An ensemble of extra-trees for</p>
<blockquote>
<div>classification</div></blockquote>
<dl class="docutils">
<dt>sklearn.ensemble.ExtraTreesRegressor <span class="classifier-delimiter">:</span> <span class="classifier">An ensemble of extra-trees for</span></dt>
<dd>regression</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreeRegressorSklearn</strong></li>
<li><strong>ExtraTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreesClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extra-trees classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><dl class="first docutils">
<dt>The number of features to consider when looking for the best split.</dt>
<dd><ul class="first last simple">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on
classification tasks and <cite>max_features=n_features</cite>
on regression problems.</li>
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</dd>
</dl>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel. If -1, then the number of jobs
is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>classes_</cite>: array of shape = [n_classes] or a list of such arrays</dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite>: int or list</dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature mportances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_decision_function_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
RandomForestClassifier : Ensemble Classifier based on trees with optimal</p>
<blockquote>
<div>splits.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreesClassifierSklearnNode</strong></li>
<li><strong>ExtraTreesClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreesRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extra-trees regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on</li>
</ul>
</li>
<li>classification tasks and <cite>max_features=n_features</cite></li>
<li>on regression problems.</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees.
Note: this parameter is tree-specific.</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel. If -1, then the number of jobs
is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature mportances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_prediction_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
RandomForestRegressor: Ensemble regressor using trees with optimal splits.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreesRegressorSklearnNode</strong></li>
<li><strong>ExtraTreesRegressorSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FactorAnalysisSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Factor Analysis (FA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.factor_analysis.FactorAnalysis</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A simple linear generative model with Gaussian latent variables.</p>
<p>The observations are assumed to be caused by a linear transformation of
lower dimensional latent factors and added Gaussian noise.
Without loss of generality the factors are distributed according to a
Gaussian with zero mean and unit covariance. The noise is also zero mean
and has an arbitrary diagonal covariance matrix.</p>
<p>If we would restrict the model further, by assuming that the Gaussian
noise is even isotropic (all diagonal entries are the same) we would obtain
<tt class="xref py py-class docutils literal"><span class="pre">PPCA</span></tt>.</p>
<p>FactorAnalysis performs a maximum likelihood estimate of the so-called
<cite>loading</cite> matrix, the transformation of the latent variables to the
observed ones, using expectation-maximization (EM).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int | None</span></dt>
<dd>Dimensionality of latent space, the number of components
of <tt class="docutils literal"><span class="pre">X</span></tt> that are obtained after <tt class="docutils literal"><span class="pre">transform</span></tt>.
If None, n_components is set to the number of features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Stopping tolerance for EM algorithm.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to make a copy of X. If <tt class="docutils literal"><span class="pre">False</span></tt>, the input X gets overwritten
during fitting.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum number of iterations.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int | bool</span></dt>
<dd>Print verbose output.</dd>
<dt>noise_variance_init <span class="classifier-delimiter">:</span> <span class="classifier">None | array, shape=(n_features,)</span></dt>
<dd>The initial guess of the noise variance for each feature.
If None, it defaults to np.ones(n_features)</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>loglike_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list, [n_iterations]</span></dt>
<dd>The log likelihood at each iteration.</dd>
<dt><cite>noise_variance_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape=(n_features,)</span></dt>
<dd>The estimated noise variance for each feature.</dd>
</dl>
<p><strong>References</strong></p>
<p>See also</p>
<dl class="docutils">
<dt>PCA: Principal component analysis, a simliar non-probabilistic</dt>
<dd>model model that can be computed in closed form.</dd>
</dl>
<p>ProbabilisticPCA: probabilistic PCA.
FastICA: Independent component analysis, a latent variable model with</p>
<blockquote>
<div>non-Gaussian latent variables.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FactorAnalysisSklearnNode</strong></li>
<li><strong>FactorAnalysisSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FeatureHasherSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Implements feature hashing, aka the hashing trick.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.hashing.FeatureHasher</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class turns sequences of symbolic feature names (strings) into
scipy.sparse matrices, using a hash function to compute the matrix column
corresponding to a name. The hash function employed is the signed 32-bit
version of Murmurhash3.</p>
<p>Feature names of type byte string are used as-is. Unicode strings are
converted to UTF-8 first, but no Unicode normalization is done.</p>
<p>This class is a low-memory alternative to DictVectorizer and
CountVectorizer, intended for large-scale (online) learning and situations
where memory is tight, e.g. when running prediction code on embedded
devices.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">NumPy type, optional</span></dt>
<dd>The type of feature values. Passed to scipy.sparse matrix constructors
as the dtype argument. Do not set this to bool, np.boolean or any
unsigned integer type.</dd>
<dt>input_type <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd>Either &#8220;dict&#8221; (the default) to accept dictionaries over
(feature_name, value); &#8220;pair&#8221; to accept pairs of (feature_name, value);
or &#8220;string&#8221; to accept single strings.
feature_name should be a string, while value should be a number.
In the case of &#8220;string&#8221;, a value of 1 is implied.
The feature_name is hashed to find the appropriate column for the
feature. The value&#8217;s sign might be flipped in the output (but see
non_negative, below).</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values will be multinomially distributed.
When False, output values will be normally distributed (Gaussian) with
mean 0, assuming a good hash function.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureHasherSklearnNode</strong></li>
<li><strong>FeatureHasherSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FeatureUnionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Concatenates results of multiple transformer objects.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pipeline.FeatureUnion</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This estimator applies a list of transformer objects in parallel to the
input data, then concatenates the results. This is useful to combine
several feature extraction mechanisms into a single transformer.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>transformers: list of (name, transformer)</dt>
<dd>List of transformer objects to be applied to the data.</dd>
<dt>n_jobs: int, optional</dt>
<dd>Number of jobs to run in parallel (default 1).</dd>
<dt>transformer_weights: dict, optional</dt>
<dd>Multiplicative weights for features per transformer.
Keys are transformer names, values the weights.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureUnionSklearn</strong></li>
<li><strong>FeatureUnionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ForestRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Base class for forest of trees-based regressors.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ForestRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ForestRegressorSklearn</strong></li>
<li><strong>ForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GaussianNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Gaussian Naive Bayes (GaussianNB)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.GaussianNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples, n_features]</span></dt>
<dd>Training vector, where n_samples in the number of samples and
n_features is the number of features.</dd>
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span></dt>
<dd>Target vector relative to X</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>class_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>probability of each class.</dd>
<dt><cite>theta_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>mean of each feature per class</dd>
<dt><cite>sigma_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>variance of each feature per class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GaussianNBSklearnNode</strong></li>
<li><strong>GaussianNBSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GenericUnivariateSelectSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Univariate feature selector with configurable strategy.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.GenericUnivariateSelect</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>mode <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;percentile&#8217;, &#8216;k_best&#8217;, &#8216;fpr&#8217;, &#8216;fdr&#8217;, &#8216;fwe&#8217;}</span></dt>
<dd>Feature selection mode.</dd>
<dt>param <span class="classifier-delimiter">:</span> <span class="classifier">float or int depending on the feature selection mode</span></dt>
<dd>Parameter of the corresponding mode.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GenericUnivariateSelectSklearn</strong></li>
<li><strong>GenericUnivariateSelectSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GradientBoostingClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Gradient Boosting for classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.GradientBoostingClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <tt class="docutils literal"><span class="pre">n_classes_</span></tt>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;deviance&#8217;}, optional (default=&#8217;deviance&#8217;)</span></dt>
<dd>loss function to be optimized. &#8216;deviance&#8217; refers to
deviance (= logistic regression) for classification
with probabilistic outputs.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span></dt>
<dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span></dt>
<dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>subsample <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, None, optional (default=None)</span></dt>
<dd>The number of features to consider when looking for the best split.
Features are choosen randomly at each split point.
If None, then <cite>max_features=n_features</cite>. Choosing
<cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator, None, optional (default=None)</span></dt>
<dd>An estimator object that is used to compute the initial
predictions. <tt class="docutils literal"><span class="pre">init</span></tt> has to provide <tt class="docutils literal"><span class="pre">fit</span></tt> and <tt class="docutils literal"><span class="pre">predict</span></tt>.
If None it uses <tt class="docutils literal"><span class="pre">loss.init_estimator</span></tt>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>Enable verbose output. If 1 then it prints &#8216;.&#8217; for every tree built.
If greater than 1 then it prints the score for every tree.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.
The i-th score <tt class="docutils literal"><span class="pre">oob_score_[i]</span></tt> is the deviance (= loss) of the
model at iteration <tt class="docutils literal"><span class="pre">i</span></tt> on the out-of-bag sample.</dd>
<dt><cite>train_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span></dt>
<dd>The i-th score <tt class="docutils literal"><span class="pre">train_score_[i]</span></tt> is the deviance (= loss) of the
model at iteration <tt class="docutils literal"><span class="pre">i</span></tt> on the in-bag sample.
If <tt class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></tt> this is the deviance on the training data.</dd>
<dt><cite>loss_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span></dt>
<dd>The concrete <tt class="docutils literal"><span class="pre">LossFunction</span></tt> object.</dd>
<dt><cite>init</cite> <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator</span></dt>
<dd>The estimator that provides the initial predictions.
Set via the <tt class="docutils literal"><span class="pre">init</span></tt> argument or <tt class="docutils literal"><span class="pre">loss.init_estimator</span></tt>.</dd>
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">samples</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">gb</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[0]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.tree.DecisionTreeClassifier, RandomForestClassifier</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GradientBoostingClassifierSklearn</strong></li>
<li><strong>GradientBoostingClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">HashingVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of text documents to a matrix of token occurrences</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.HashingVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>It turns a collection of text documents into a scipy.sparse matrix holding
token occurrence counts (or binary occurrence information), possibly
normalized as token frequencies if norm=&#8217;l1&#8217; or projected on the euclidean
unit sphere if norm=&#8217;l2&#8217;.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantage:</p>
<ul class="simple">
<li>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory</li>
<li>it is fast to pickle and un-pickle has it holds no state besides the
constructor parameters</li>
<li>it can be used in a streaming (partial fit) or parallel pipeline as there
is no state computed during fit.</li>
</ul>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<ul class="simple">
<li>there is no way to compute the inverse transform (from feature indices to
string feature names) which can be a problem when trying to introspect
which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.</li>
</ul>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input: string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>charset: string, &#8216;utf-8&#8217; by default.</dt>
<dd>If bytes or files are given to analyze, this charset is used to
decode.</dd>
<dt>charset_error: {&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>charset</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents: {&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer: string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor: callable or None (default)</dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer: callable or None (default)</dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range: tuple (min_n, max_n)</dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words: string {&#8216;english&#8217;}, list, or None (default)</dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned is currently the only
supported string value.</p>
<p class="last">If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
</dd>
<dt>lowercase: boolean, default True</dt>
<dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>token_pattern: string</dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">interger, optional, (2 ** 20) by default</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>binary: boolean, False by default.</dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype: type, optional</dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values will be multinomially distributed.
When False, output values will be normally distributed (Gaussian) with
mean 0, assuming a good hash function.</dd>
</dl>
<p>See also</p>
<p>CountVectorizer, TfidfVectorizer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>HashingVectorizerSklearn</strong></li>
<li><strong>HashingVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-isomapsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-isomapsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">IsomapSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Isomap Embedding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.manifold.isomap.Isomap</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Non-linear dimensionality reduction through Isometric Mapping</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">[&#8216;auto&#8217;|&#8217;arpack&#8217;|&#8217;dense&#8217;]</span></dt>
<dd><dl class="first last docutils">
<dt>&#8216;auto&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Attempt to choose the most efficient solver</span></dt>
<dd>for the given problem.</dd>
<dt>&#8216;arpack&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Use Arnoldi decomposition to find the eigenvalues</span></dt>
<dd>and eigenvectors.</dd>
<dt>&#8216;dense&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Use a direct solver (i.e. LAPACK)</span></dt>
<dd>for the eigenvalue decomposition.</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance passed to arpack or lobpcg.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Maximum number of iterations for the arpack solver.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>path_method <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;FW&#8217;|&#8217;D&#8217;]</span></dt>
<dd>Method to use in finding shortest path.
&#8216;auto&#8217; : attempt to choose the best algorithm automatically
&#8216;FW&#8217; : Floyd-Warshall algorithm
&#8216;D&#8217; : Dijkstra algorithm with Fibonacci Heaps</dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span></dt>
<dd>Algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>embedding_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_components)</span></dt>
<dd>Stores the embedding vectors.</dd>
<dt><cite>kernel_pca_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><cite>KernelPCA</cite> object used to implement the embedding.</dd>
<dt><cite>training_data_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_features)</span></dt>
<dd>Stores the training data.</dd>
<dt><cite>nbrs_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">sklearn.neighbors.NearestNeighbors instance</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
<dt><cite>dist_matrix_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_samples)</span></dt>
<dd>Stores the geodesic distance matrix of training data.</dd>
</dl>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C. A global geometric</dt>
<dd>framework for nonlinear dimensionality reduction. Science 290 (5500)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>IsomapSklearnNode</strong></li>
<li><strong>IsomapSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">IsotonicRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Isotonic regression model.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.isotonic.IsotonicRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The isotonic regression optimization problem is defined by:</p>
<div class="highlight-python"><pre>min sum w_i (y[i] - y_[i]) ** 2

subject to y_[i] &lt;= y_[j] whenever X[i] &lt;= X[j]
and min(y_) = y_min, max(y_) = y_max</pre>
</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">y[i]</span></tt> are inputs (real numbers)</li>
</ul>
</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">y_[i]</span></tt> are fitted</li>
</ul>
</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">X</span></tt> specifies the order.</li>
</ul>
</li>
<li>If <tt class="docutils literal"><span class="pre">X</span></tt> is non-decreasing then <tt class="docutils literal"><span class="pre">y_</span></tt> is non-decreasing.</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">w[i]</span></tt> are optional strictly positive weights (default to 1.0)</li>
</ul>
</li>
</ul>
</div></blockquote>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>y_min <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the lowest value of the fit to y_min.</dd>
<dt>y_max <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the highest value of the fit to y_max.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>X_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span></dt>
<dd>A copy of the input X.</dd>
<dt><cite>y_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span></dt>
<dd>Isotonic fit of y.</dd>
</dl>
<p><strong>References</strong></p>
<p>Isotonic Median Regression: A Linear Programming Approach
Nilotpal Chakravarti
Mathematics of Operations Research
Vol. 14, No. 2 (May, 1989), pp. 303-308</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>IsotonicRegressionSklearn</strong></li>
<li><strong>IsotonicRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KNeighborsClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier implementing the k-nearest neighbors vote.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.classification.KNeighborsClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span></dt>
<dd>Number of neighbors to use by default for <tt class="xref py py-meth docutils literal"><span class="pre">k_neighbors()</span></tt> queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">BallTree</span></tt></li>
<li>&#8216;kd_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">scipy.spatial.cKDtree</span></tt></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <tt class="xref py py-meth docutils literal"><span class="pre">fit()</span></tt> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or cKDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>warn_on_equidistant <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional.  Defaults to True.</span></dt>
<dd>Generate a warning if equidistant neighbors are discarded.
For classification or regression based on k-neighbors, if
neighbor k and neighbor k+1 have identical distances but
different labels, then the result will be dependent on the
ordering of the training data.
If the fit method is <tt class="docutils literal"><span class="pre">'kd_tree'</span></tt>, no warnings will be generated.</dd>
<dt>p: integer, optional (default = 2)</dt>
<dd>Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="go">[[ 0.66666667  0.33333333]]</span>
</pre></div>
</div>
<p>See also</p>
<p>RadiusNeighborsClassifier
KNeighborsRegressor
RadiusNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <em class="xref std std-ref">Nearest Neighbors</em> in the online documentation
for a discussion of the choice of <tt class="docutils literal"><span class="pre">algorithm</span></tt> and <tt class="docutils literal"><span class="pre">leaf_size</span></tt>.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KNeighborsClassifierSklearnNode</strong></li>
<li><strong>KNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KernelCentererSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Center a kernel matrix</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.KernelCenterer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Let K(x_i, x_j) be a kernel defined by K(x_i, x_j) = phi(x_i)^T phi(x_j),
where phi(x) is a function mapping x to a hilbert space. KernelCenterer is
a class to center (i.e., normalize to have zero-mean) the data without
explicitly computing phi(x). It is equivalent equivalent to centering
phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KernelCentererSklearnNode</strong></li>
<li><strong>KernelCentererSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KernelPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Kernel Principal component analysis (KPCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.kernel_pca.KernelPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Non-linear dimensionality reduction through the use of kernels.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int or None</dt>
<dd>Number of components. If None, all non-zero components are kept.</dd>
<dt>kernel: &#8220;linear&#8221; | &#8220;poly&#8221; | &#8220;rbf&#8221; | &#8220;sigmoid&#8221; | &#8220;cosine&#8221; | &#8220;precomputed&#8221;</dt>
<dd>Kernel.
Default: &#8220;linear&#8221;</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Degree for poly, rbf and sigmoid kernels.
Default: 3.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Kernel coefficient for rbf and poly kernels.
Default: 1/n_features.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Independent term in poly and sigmoid kernels.</dd>
<dt>alpha: int</dt>
<dd>Hyperparameter of the ridge regression that learns the
inverse transform (when fit_inverse_transform=True).
Default: 1.0</dd>
<dt>fit_inverse_transform: bool</dt>
<dd>Learn the inverse transform for non-precomputed kernels.
(i.e. learn to find the pre-image of a point)
Default: False</dd>
<dt>eigen_solver: string [&#8216;auto&#8217;|&#8217;dense&#8217;|&#8217;arpack&#8217;]</dt>
<dd>Select eigensolver to use.  If n_components is much less than
the number of training samples, arpack may be more efficient
than the dense eigensolver.</dd>
<dt>tol: float</dt>
<dd>convergence tolerance for arpack.
Default: 0 (optimal value will be chosen by arpack)</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>maximum number of iterations for arpack
Default: None (optimal value will be chosen by arpack)</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>lambdas_</cite>, <cite>alphas_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Eigenvalues and eigenvectors of the centered kernel matrix</li>
</ul>
</div></blockquote>
<p><cite>dual_coef_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Inverse transform matrix</li>
</ul>
</div></blockquote>
<p><cite>X_transformed_fit_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Projection of the fitted data on the kernel principal components</li>
</ul>
</div></blockquote>
<p><strong>References</strong></p>
<p>Kernel PCA was intoduced in:</p>
<blockquote>
<div><ul class="simple">
<li>Bernhard Schoelkopf, Alexander J. Smola,</li>
<li>and Klaus-Robert Mueller. 1999. Kernel principal</li>
<li>component analysis. In Advances in kernel methods,</li>
<li>MIT Press, Cambridge, MA, USA 327-352.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KernelPCASklearn</strong></li>
<li><strong>KernelPCASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ldasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LDASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LDASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LDASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ldasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LDASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LDASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LDASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear Discriminant Analysis (LDA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.lda.LDA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A classifier with a linear decision boundary, generated
by fitting class conditional densities to the data
and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class, assuming that
all classes share the same covariance matrix.</p>
<p>The fitted model can also be used to reduce the dimensionality
of the input, by projecting it to the most discriminative
directions.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int</dt>
<dd>Number of components (&lt; n_classes - 1) for dimensionality reduction</dd>
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span></dt>
<dd>Priors on classes</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [rank, n_classes - 1]</span></dt>
<dd>Coefficients of the features in the linear decision
function. rank is min(rank_features, n_classes) where
rank_features is the dimensionality of the spaces spanned
by the features (i.e. n_features excluding redundant features).</dd>
<dt><cite>covariance_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_features, n_features]</span></dt>
<dd>Covariance matrix (shared by all classes).</dd>
<dt><cite>means_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Class means.</dd>
<dt><cite>priors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><cite>scalings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [rank, n_classes - 1]</span></dt>
<dd>Scaling of the features in the space spanned by the class
centroids.</dd>
<dt><cite>xbar_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float, shape = [n_features]</span></dt>
<dd>Overall mean.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.lda</span> <span class="kn">import</span> <span class="n">LDA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LDA(n_components=None, priors=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.qda.QDA: Quadratic discriminant analysis</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LDASklearnNode</strong></li>
<li><strong>LDASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelBinarizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Binarize labels in a one-vs-all fashion</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.LabelBinarizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Several regression and binary classification algorithms are
available in the scikit. A simple way to extend these algorithms
to the multi-class classification case is to use the so-called
one-vs-all scheme.</p>
<p>At learning time, this simply consists in learning one regressor
or binary classifier per class. In doing so, one needs to convert
multi-class labels to binary labels (belong or does not belong
to the class). LabelBinarizer makes this process easy with the
transform method.</p>
<p>At prediction time, one assigns the class for which the corresponding
model gave the greatest confidence. LabelBinarizer makes this easy
with the inverse_transform method.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>neg_label: int (default: 0)</dt>
<dd>Value with which negative labels must be encoded.</dd>
<dt>pos_label: int (default: 1)</dt>
<dd>Value with which positive labels must be encoded.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>classes_</cite>: array of shape [n_class]</dt>
<dd>Holds the label for each class.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 4, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1]])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span>
<span class="go">array([[1, 1, 0],</span>
<span class="go">       [0, 0, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 3])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelBinarizerSklearn</strong></li>
<li><strong>LabelBinarizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelEncoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Encode labels with value between 0 and n_classes-1.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.LabelEncoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>classes_</cite>: array of shape [n_class]</dt>
<dd>Holds the label for each class.</dd>
</dl>
<p><strong>Examples</strong></p>
<p><cite>LabelEncoder</cite> can be used to normalize labels.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> 
<span class="go">array([0, 0, 1, 2]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">array([1, 1, 2, 6])</span>
</pre></div>
</div>
<p>It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;amsterdam&#39;, &#39;paris&#39;, &#39;tokyo&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">])</span> 
<span class="go">array([2, 2, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">[&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelEncoderSklearn</strong></li>
<li><strong>LabelEncoderSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelPropagationSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Label Propagation classifier</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelPropagation</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span></dt>
<dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported..</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>clamping factor</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>change maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelPropagation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelPropagation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelPropagation(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data
with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002 <a class="reference external" href="http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf">http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf</a></p>
<p>See Also</p>
<p>LabelSpreading : Alternate label proagation strategy more robust to noise</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelPropagationSklearn</strong></li>
<li><strong>LabelPropagationSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelSpreadingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>LabelSpreading model for semi-supervised learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelSpreading</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This model is similar to the basic Label Propgation algorithm,
but uses affinity matrix based on the normalized graph Laplacian
and soft clamping across the labels.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span></dt>
<dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>clamping factor</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelSpreading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelSpreading</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelSpreading(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,
Bernhard Schölkopf. Learning with local and global consistency (2004)
<a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219</a></p>
<p>See Also</p>
<p>LabelPropagation : Unregularized graph based semi-supervised learning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelSpreadingSklearnNode</strong></li>
<li><strong>LabelSpreadingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LinearSVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LinearSVCSklearn</strong></li>
<li><strong>LinearSVCSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LocallyLinearEmbeddingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Locally Linear Embedding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.manifold.locally_linear.LocallyLinearEmbedding</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>reg <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>regularization constant, multiplies the trace of the local covariance
matrix of the distances.</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;auto&#8217;, &#8216;arpack&#8217;, &#8216;dense&#8217;}</span></dt>
<dd><p class="first">auto : algorithm will attempt to choose the best method for input data</p>
<dl class="last docutils">
<dt>arpack <span class="classifier-delimiter">:</span> <span class="classifier">use arnoldi iteration in shift-invert mode.</span></dt>
<dd>For this method, M may be a dense matrix, sparse matrix,
or general linear operator.
Warning: ARPACK can be unstable for some problems.  It is
best to try several random seeds in order to check results.</dd>
<dt>dense <span class="classifier-delimiter">:</span> <span class="classifier">use standard dense matrix operations for the eigenvalue</span></dt>
<dd>decomposition.  For this method, M must be an array
or matrix type.  This method should be avoided for
large problems.</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for &#8216;arpack&#8217; method
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>maximum number of iterations for the arpack solver.
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;standard&#8217; | &#8216;hessian&#8217; | &#8216;modified&#8217; <a href="#id16"><span class="problematic" id="id17">|</span></a>&#8216;ltsa&#8217;]</span></dt>
<dd><dl class="first last docutils">
<dt>standard <span class="classifier-delimiter">:</span> <span class="classifier">use the standard locally linear embedding algorithm.</span></dt>
<dd>see reference [1]</dd>
<dt>hessian <span class="classifier-delimiter">:</span> <span class="classifier">use the Hessian eigenmap method.  This method requires</span></dt>
<dd>n_neighbors &gt; n_components * (1 + (n_components + 1) / 2.
see reference [2]</dd>
<dt>modified <span class="classifier-delimiter">:</span> <span class="classifier">use the modified locally linear embedding algorithm.</span></dt>
<dd>see reference [3]</dd>
<dt>ltsa <span class="classifier-delimiter">:</span> <span class="classifier">use local tangent space alignment algorithm</span></dt>
<dd>see reference [4]</dd>
</dl>
</dd>
<dt>hessian_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for Hessian eigenmapping method.
Only used if method == &#8216;hessian&#8217;</dd>
<dt>modified_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for modified LLE method.
Only used if method == &#8216;modified&#8217;</dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span></dt>
<dd>algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance</dd>
<dt>random_state: numpy.RandomState or int, optional</dt>
<dd>The generator or seed used to determine the starting vector for arpack
iterations.  Defaults to numpy.random.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>embedding_vectors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape [n_components, n_samples]</span></dt>
<dd>Stores the embedding vectors</dd>
<dt><cite>reconstruction_error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Reconstruction error associated with <cite>embedding_vectors_</cite></dd>
<dt><cite>nbrs_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">NearestNeighbors object</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><cite>Roweis, S. &amp; Saul, L. Nonlinear dimensionality reduction
by locally linear embedding.  Science 290:2323 (2000).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><cite>Donoho, D. &amp; Grimes, C. Hessian eigenmaps: Locally
linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A.  100:5591 (2003).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><cite>Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear
Embedding Using Multiple Weights.</cite>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><cite>Zhang, Z. &amp; Zha, H. Principal manifolds and nonlinear
dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ.  8:406 (2004)</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LocallyLinearEmbeddingSklearnNode</strong></li>
<li><strong>LocallyLinearEmbeddingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LogisticRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Logistic Regression (aka logit, MaxEnt) classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.logistic.LogisticRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>In the multiclass case, the training algorithm uses a one-vs.-all (OvA)
scheme, rather than the &#8220;true&#8221; multinomial LR.</p>
<p>This class implements L1 and L2 regularized logistic regression using the
<cite>liblinear</cite> library. It can handle both dense and sparse input. Use
C-ordered arrays or CSR matrices containing 64-bit floats for optimal
performance; any other input format will be converted (and copied).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;l1&#8217; or &#8216;l2&#8217;</span></dt>
<dd>Used to specify the norm used in the penalization.</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Dual or primal formulation. Dual formulation is only
implemented for l2 penalty. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Specifies the strength of the regularization. The smaller it is
the bigger is the regularization.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span></dt>
<dd>Specifies if a constant (a.k.a. bias or intercept) should be
added the decision function.</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1</span></dt>
<dd>when self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The &#8216;auto&#8217; mode uses the values of y to
automatically adjust weights inversely proportional to
class frequencies.</dd>
<dt>tol: float, optional</dt>
<dd>Tolerance for stopping criteria.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes-1, n_features]</span></dt>
<dd><p class="first">Coefficient of the features in the decision function.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>raw_coef_</cite> that         follows the internal memory layout of liblinear.</p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes-1]</span></dt>
<dd>Intercept (a.k.a. bias) added to the decision function.
It is available only when parameter intercept is set to True.</dd>
</dl>
<p>See also</p>
<p>LinearSVC</p>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>References:</p>
<dl class="docutils">
<dt>LIBLINEAR &#8211; A Library for Large Linear Classification</dt>
<dd><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">http://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></dd>
<dt>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</dt>
<dd>methods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LogisticRegressionSklearn</strong></li>
<li><strong>LogisticRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MinMaxScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Standardizes features by scaling each feature to a given range.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.MinMaxScaler</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This estimator scales and translates each feature individually such
that it is in the given range on the training set, i.e. between
zero and one.</p>
<p>The standardization is given by:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="o">-</span> <span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="o">-</span> <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>This standardization is often used as an alternative to zero mean,
unit variance scaling.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>feature_range: tuple (min, max), default=(0, 1)</dt>
<dd>Desired range of transformed data.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>Set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>min_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature adjustment for minimum.</dd>
<dt><cite>scale_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature relative scaling of the data.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MinMaxScalerSklearnNode</strong></li>
<li><strong>MinMaxScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MiniBatchDictionaryLearningSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Mini-batch dictionary learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-python"><pre>(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
             (U,V)
             with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components</pre>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>total number of iterations to perform</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd>Algorithm used to transform the data.
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection dictionary * X&#8217;</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial value of the dictionary for warm restart scenarios</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of samples in each mini-batch</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool,</span></dt>
<dd>whether to shuffle the samples before forming batches</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>components extracted from the data</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
DictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MiniBatchDictionaryLearningSklearn</strong></li>
<li><strong>MiniBatchDictionaryLearningSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MiniBatchSparsePCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Mini-batch Sparse Principal Components Analysis</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.MiniBatchSparsePCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of sparse atoms to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of iterations to perform for each mini batch</dd>
<dt>callback <span class="classifier-delimiter">:</span> <span class="classifier">callable,</span></dt>
<dd>callable that gets invoked every five iterations</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>the number of features to take in each mini batch</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of output the procedure will print</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span></dt>
<dd>whether to shuffle the data before splitting it in batches</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run, or -1 to autodetect.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Vector of errors at each iteration.</dd>
</dl>
<p>See also</p>
<p>PCA
SparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MiniBatchSparsePCASklearnNode</strong></li>
<li><strong>MiniBatchSparsePCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MultinomialNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Naive Bayes classifier for multinomial models</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.MultinomialNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size=[n_classes,]</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>intercept_</cite>, <cite>class_log_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Smoothed empirical log probability for each class.</dd>
<dt><cite>feature_log_prob_</cite>, <cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd><p class="first">Empirical log probability of features
given a class, P(x_i|y).</p>
<p class="last">(<cite>intercept_</cite> and <cite>coef_</cite> are properties
referring to <cite>class_log_prior_</cite> and
<cite>feature_log_prob_</cite>, respectively.)</p>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For the rationale behind the names <cite>coef_</cite> and <cite>intercept_</cite>, i.e.
naive Bayes as a linear classifier, see J. Rennie et al. (2003),
Tackling the poor assumptions of naive Bayes text classifiers, ICML.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MultinomialNBSklearn</strong></li>
<li><strong>MultinomialNBSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nmfsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nmfsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NMFSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Non-Negative matrix factorization by Projected Gradient (NMF)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.nmf.NMF</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int or None</dt>
<dd>Number of components, if n_components is not set all components
are kept</dd>
<dt>init:  &#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;random&#8217;</dt>
<dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<div class="last highlight-python"><pre>'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
    initialization (better for sparseness)
'nndsvda': NNDSVD with zeros filled with the average of X
    (better when sparsity is not desired)
'nndsvdar': NNDSVD with zeros filled with small random values
    (generally faster, less accurate alternative to NNDSVDa
    for when sparsity is not desired)
'random': non-negative random matrices</pre>
</div>
</dd>
<dt>sparseness: &#8216;data&#8217; | &#8216;components&#8217; | None, default: None</dt>
<dd>Where to enforce sparsity in the model.</dd>
<dt>beta: double, default: 1</dt>
<dd>Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness.</dd>
<dt>eta: double, default: 0.1</dt>
<dd>Degree of correctness to mantain, if sparsity is not None. Smaller
values mean larger error.</dd>
<dt>tol: double, default: 1e-4</dt>
<dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter: int, default: 200</dt>
<dd>Number of iterations to compute.</dd>
<dt>nls_max_iter: int, default: 2000</dt>
<dd>Number of iterations in NLS subproblem.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Random number generator seed control.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Non-negative components of the data</dd>
<dt><cite>reconstruction_err_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">number</span></dt>
<dd>Frobenius norm of the matrix difference between the
training data and the reconstructed data from the
fit produced by the model. <tt class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></tt>
Not computed for sparse input matrices because it is
too expensive in terms of memory.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">ProjectedGradientNMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">        n_components=2, nls_max_iter=2000, random_state=0, sparseness=None,</span>
<span class="go">        tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 0.77032744,  0.11118662],</span>
<span class="go">       [ 0.38526873,  0.38228063]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00746...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">sparseness</span><span class="o">=</span><span class="s">&#39;components&#39;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">            n_components=2, nls_max_iter=2000, random_state=0,</span>
<span class="go">            sparseness=&#39;components&#39;, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 1.67481991,  0.29614922],</span>
<span class="go">       [-0.        ,  0.4681982 ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.513...</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>This implements</p>
<p>C.-J. Lin. Projected gradient methods
for non-negative matrix factorization. Neural
Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>P. Hoyer. Non-negative Matrix Factorization with
Sparseness Constraints. Journal of Machine Learning
Research 2004.</p>
<p>NNDSVD is introduced in</p>
<p>C. Boutsidis, E. Gallopoulos: SVD based
initialization: A head start for nonnegative
matrix factorization - Pattern Recognition, 2008
<a class="reference external" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NMFSklearn</strong></li>
<li><strong>NMFSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NearestCentroidSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Nearest centroid classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.nearest_centroid.NearestCentroid</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>metric: string, or callable</dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string or callable, it must be one of
the options allowed by metrics.pairwise.pairwise_distances for its
metric parameter.</dd>
<dt>shrink_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = None)</span></dt>
<dd>Threshold for shrinking centroids to remove features.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>centroids_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Centroid of each class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier</p>
<p><strong>Notes</strong></p>
<p>When used for text classification with tf–idf vectors, this classifier is
also known as the Rocchio classifier.</p>
<p><strong>References</strong></p>
<p>Tibshirani, R., Hastie, T., Narasimhan, B., &amp; Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NearestCentroidSklearnNode</strong></li>
<li><strong>NearestCentroidSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-normalizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-normalizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NormalizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Normalize samples individually to unit norm</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.Normalizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1 or l2) equals one.</p>
<p>This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).</p>
<p>Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217; or &#8216;l2&#8217;, optional (&#8216;l2&#8217; by default)</span></dt>
<dd>The norm to use to normalize each non zero sample.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p>See also</p>
<p><tt class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.normalize()</span></tt> equivalent function
without the object oriented API</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NormalizerSklearn</strong></li>
<li><strong>NormalizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NuSVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>NuSVC for sparse matrices (csr).</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.svm.sparse.classes.NuSVC</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>See <tt class="xref py py-class docutils literal"><span class="pre">sklearn.svm.NuSVC</span></tt> for a complete list of parameters</p>
<p><strong>Notes</strong></p>
<p>For best results, this accepts a matrix in csr format
(scipy.sparse.csr), but should be able to convert from any array-like
object (including other sparse representations).</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm.sparse</span> <span class="kn">import</span> <span class="n">NuSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">NuSVC(cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,</span>
<span class="go">        kernel=&#39;rbf&#39;, max_iter=-1, nu=0.5, probability=False,</span>
<span class="go">        shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NuSVCSklearnNode</strong></li>
<li><strong>NuSVCSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">OneHotEncoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Encode categorical integer features using a one-hot aka one-of-K scheme.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The input to this transformer should be a matrix of integers, denoting
the values taken on by categorical (discrete) features. The output will be
a sparse matrix were each column corresponds to one possible value of one
feature. It is assumed that input features take on values in the range
[0, n_values).</p>
<p>This encoding is needed for feeding categorical data to scikit-learn
estimators.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_values <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;auto&#8217;, int or array of int</span></dt>
<dd>Number of values per feature.
&#8216;auto&#8217; : determine value range from training data.
int : maximum value for all features.
array : maximum value per feature.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">number type, default=np.float</span></dt>
<dd>Desired dtype of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>active_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Indices for active features, meaning values that actually occur
in the training set. Only available when n_values is <tt class="docutils literal"><span class="pre">'auto'</span></tt>.</dd>
<dt><cite>feature_indices_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd>Indices to feature ranges.
Feature <tt class="docutils literal"><span class="pre">i</span></tt> in the original data is mapped to features
from <tt class="docutils literal"><span class="pre">feature_indices_[i]</span></tt> to <tt class="docutils literal"><span class="pre">feature_indices_[i+1]</span></tt>
(and then potentially masked by <cite>active_features_</cite> afterwards)</dd>
<dt><cite>n_values_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd>Maximum number of values per feature.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Given a dataset with three features and two samples, we let the encoder
find the maximum value per feature and transform the data to a binary
one-hot encoding.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">OneHotEncoder(dtype=&lt;type &#39;float&#39;&gt;, n_values=&#39;auto&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">n_values_</span>
<span class="go">array([2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">feature_indices_</span>
<span class="go">array([0, 2, 5, 9])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])</span>
</pre></div>
</div>
<p>See also</p>
<p>LabelEncoder : performs a one-hot encoding on arbitrary class labels.
sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of</p>
<blockquote>
<div>dictionary items (also handles string-valued features).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>OneHotEncoderSklearnNode</strong></li>
<li><strong>OneHotEncoderSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-pcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-pcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Principal component analysis (PCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.PCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data and keeping only the most significant singular vectors to project the
data to a lower dimensional space.</p>
<p>This implementation uses the scipy.linalg implementation of the singular
value decomposition. It only works for dense arrays and is not scalable to
large dimensional data.</p>
<p>The time complexity of this implementation is <tt class="docutils literal"><span class="pre">O(n</span> <span class="pre">**</span> <span class="pre">3)</span></tt> assuming
n ~ n_samples ~ n_features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, None or string</span></dt>
<dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">if n_components == &#8216;mle&#8217;, Minka&#8217;s MLE is used to guess the dimension
if <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></tt>, select the number of components such that
the amount of variance that needs to be explained is greater than the
percentage specified by n_components</p>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by n_samples times singular values to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making there data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For n_components=&#8217;mle&#8217;, this class uses the method of <a href="#id22"><span class="problematic" id="id23">`</span></a>Thomas P. Minka:</p>
<p>Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`</p>
<p>Due to implementation subtleties of the Singular Value Decomposition (SVD),
which is used in this implementation, running fit twice on the same matrix
can lead to principal components with signs flipped (change in direction).
For this reason, it is important to always use the same estimator object to
transform data in a consistent fashion.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>ProbabilisticPCA
RandomizedPCA
KernelPCA
SparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PCASklearn</strong></li>
<li><strong>PCASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-plscanonicalsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-plscanonicalsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PLSCanonicalSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PLSCanonicalSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>PLSCanonical implements the 2 blocks canonical PLS of the original Wold
algorithm [Tenenhaus 1998] p.204, refered as PLS-C2A in [Wegelin 2000].</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pls.PLSCanonical</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class inherits from PLS with mode=&#8221;A&#8221; and deflation_mode=&#8221;canonical&#8221;,
norm_y_weights=True and algorithm=&#8221;nipals&#8221;, but svd should provide similar
results up to numerical errors.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like of predictors, shape = [n_samples, p]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
p is the number of predictors.</dd>
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">array-like of response, shape = [n_samples, q]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
q is the number of response variables.</dd>
</dl>
<p>n_components : int, number of components to keep. (default 2).</p>
<p>scale : boolean, scale data? (default True)</p>
<dl class="docutils">
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8220;nipals&#8221; or &#8220;svd&#8221;</span></dt>
<dd>The algorithm used to estimate the weights. It will be called
n_components times, i.e. once for each iteration of the outer loop.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop (used
only if algorithm=&#8221;nipals&#8221;)</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real, default 1e-06</span></dt>
<dd>the tolerance used in the iterative algorithm</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether the deflation should be done on a copy. Let the default
value to True unless you don&#8217;t care about side effect</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>x_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><cite>y_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><cite>x_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><cite>y_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><cite>x_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><cite>y_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><cite>x_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><cite>y_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For each component k, find weights u, v that optimize:</p>
<p>max corr(Xk u, Yk v) * var(Xk u) var(Yk u), such that <tt class="docutils literal"><span class="pre">|u|</span> <span class="pre">=</span> <span class="pre">|v|</span> <span class="pre">=</span> <span class="pre">1</span></tt></p>
<p>Note that it maximizes both the correlations between the scores and the
intra-block variances.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on the
current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current Y score. This performs a canonical symetric version of the PLS
regression. But slightly different than the CCA. This is mode mostly used
for modeling.</p>
<p>This implementation provides the same results that the &#8220;plspm&#8221; package
provided in the R language (R-project), using the function plsca(X, Y).
Results are equal or colinear with the function
<tt class="docutils literal"><span class="pre">pls(...,</span> <span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;canonical&quot;)</span></tt> of the &#8220;mixOmics&#8221; package. The difference
relies in the fact that mixOmics implmentation does not exactly implement
the Wold algorithm since it does not normalize y_weights to one.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pls</span> <span class="kn">import</span> <span class="n">PLSCanonical</span><span class="p">,</span> <span class="n">PLSRegression</span><span class="p">,</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span> <span class="o">=</span> <span class="n">PLSCanonical</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plsca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">PLSCanonical(algorithm=&#39;nipals&#39;, copy=True, max_iter=500, n_components=2,</span>
<span class="go">             scale=True, tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_c</span><span class="p">,</span> <span class="n">Y_c</span> <span class="o">=</span> <span class="n">plsca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<p>See also</p>
<p>CCA
PLSSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PLSCanonicalSklearnNode</strong></li>
<li><strong>PLSCanonicalSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-plsregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-plsregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PLSRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PLSRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>PLS regression</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pls.PLSRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1
in case of one dimensional response.
This class inherits from _PLS with mode=&#8221;A&#8221;, deflation_mode=&#8221;regression&#8221;,
norm_y_weights=False and algorithm=&#8221;nipals&#8221;.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like of predictors, shape = [n_samples, p]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
p is the number of predictors.</dd>
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">array-like of response, shape = [n_samples, q]</span></dt>
<dd>Training vectors, where n_samples in the number of samples and
q is the number of response variables.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2)</span></dt>
<dd>Number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>whether to scale the data</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">an integer, (default 500)</span></dt>
<dd>the maximum number of iterations of the NIPALS inner loop (used
only if algorithm=&#8221;nipals&#8221;)</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">non-negative real</span></dt>
<dd>Tolerance used in the iterative algorithm default 1e-06.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Whether the deflation should be done on a copy. Let the default
value to True unless you don&#8217;t care about side effect</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>x_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><cite>y_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><cite>x_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block loadings vectors.</dd>
<dt><cite>y_loadings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block loadings vectors.</dd>
<dt><cite>x_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><cite>y_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
<dt><cite>x_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block to latents rotations.</dd>
<dt><cite>y_rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block to latents rotations.</dd>
<dt>coefs: array, [p, q]</dt>
<dd>The coeficients of the linear model: Y = X coefs + Err</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For each component k, find weights u, v that optimizes:</p>
<p><tt class="docutils literal"><span class="pre">max</span> <span class="pre">corr(Xk</span> <span class="pre">u,</span> <span class="pre">Yk</span> <span class="pre">v)</span> <span class="pre">*</span> <span class="pre">var(Xk</span> <span class="pre">u)</span> <span class="pre">var(Yk</span> <span class="pre">u)</span></tt>, such that <tt class="docutils literal"><span class="pre">|u|</span> <span class="pre">=</span> <span class="pre">1</span></tt></p>
<p>Note that it maximizes both the correlations between the scores and the
intra-block variances.</p>
<p>The residual matrix of X (Xk+1) block is obtained by the deflation on
the current X score: x_score.</p>
<p>The residual matrix of Y (Yk+1) block is obtained by deflation on the
current X score. This performs the PLS regression known as PLS2. This
mode is prediction oriented.</p>
<p>This implementation provides the same results that 3 PLS packages
provided in the R language (R-project):</p>
<blockquote>
<div><ul class="simple">
<li>&#8220;mixOmics&#8221; with function pls(X, Y, mode = &#8220;regression&#8221;)</li>
<li>&#8220;plspm &#8221; with function plsreg2(X, Y)</li>
<li>&#8220;pls&#8221; with function oscorespls.fit(X, Y)</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pls</span> <span class="kn">import</span> <span class="n">PLSCanonical</span><span class="p">,</span> <span class="n">PLSRegression</span><span class="p">,</span> <span class="n">CCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pls2</span> <span class="o">=</span> <span class="n">PLSRegression</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pls2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,</span>
<span class="go">        tol=1e-06)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">pls2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with
emphasis on the two-block case. Technical Report 371, Department of
Statistics, University of Washington, Seattle, 2000.</p>
<p>In french but still a reference:</p>
<p>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:</p>
<p>Editions Technic.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PLSRegressionSklearnNode</strong></li>
<li><strong>PLSRegressionSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-plssvdsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-plssvdsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PLSSVDSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PLSSVDSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Partial Least Square SVD</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pls.PLSSVD</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Simply perform a svd on the crosscovariance matrix: X&#8217;Y
The are no iterative deflation here.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like of predictors, shape = [n_samples, p]</span></dt>
<dd>Training vector, where n_samples in the number of samples and
p is the number of predictors. X will be centered before any analysis.</dd>
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">array-like of response, shape = [n_samples, q]</span></dt>
<dd>Training vector, where n_samples in the number of samples and
q is the number of response variables. X will be centered before any
analysis.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, (default 2).</span></dt>
<dd>number of components to keep.</dd>
<dt>scale <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default True)</span></dt>
<dd>scale X and Y</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>x_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [p, n_components]</span></dt>
<dd>X block weights vectors.</dd>
<dt><cite>y_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [q, n_components]</span></dt>
<dd>Y block weights vectors.</dd>
<dt><cite>x_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>X scores.</dd>
<dt><cite>y_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_samples, n_components]</span></dt>
<dd>Y scores.</dd>
</dl>
<p>See also</p>
<p>PLSCanonical
CCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PLSSVDSklearn</strong></li>
<li><strong>PLSSVDSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PassiveAggressiveClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Passive Aggressive Classifier</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>hinge: equivalent to PA-I in the reference paper.</li>
<li>squared_hinge: equivalent to PA-II in the reference paper.</li>
</ul>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p>See also</p>
<p>SGDClassifier
Perceptron</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PassiveAggressiveClassifierSklearn</strong></li>
<li><strong>PassiveAggressiveClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PatchExtractorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Extracts patches from a collection of images</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.image.PatchExtractor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>patch_size: tuple of ints (patch_height, patch_width)</dt>
<dd>the dimensions of one patch</dd>
<dt>max_patches: integer or float, optional default is None</dt>
<dd>The maximum number of patches per image to extract. If max_patches is a
float in (0, 1), it is taken to mean a proportion of the total number
of patches.</dd>
<dt>random_state: int or RandomState</dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PatchExtractorSklearn</strong></li>
<li><strong>PatchExtractorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PerceptronSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Perceptron</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.perceptron.Perceptron</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">None, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to None.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term if regularization is
used. Defaults to 0.0001</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>Constant by which the updates are multiplied. Defaults to 1.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label</span> <span class="classifier-delimiter">:</span> <span class="classifier">weight} or &#8220;auto&#8221; or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;auto&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies.</p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><cite>Perceptron</cite> and <cite>SGDClassifier</cite> share the same underlying implementation.
In fact, <cite>Perceptron()</cite> is equivalent to <cite>SGDClassifier(loss=&#8221;perceptron&#8221;,
eta0=1, learning_rate=&#8221;constant&#8221;, penalty=None)</cite>.</p>
<p>See also</p>
<p>SGDClassifier</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Perceptron">http://en.wikipedia.org/wiki/Perceptron</a> and references therein.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PerceptronSklearn</strong></li>
<li><strong>PerceptronSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PipelineSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Pipeline of transforms with a final estimator.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pipeline.Pipeline</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Sequentially apply a list of transforms and a final estimator.
Intermediate steps of the pipeline must be &#8216;transforms&#8217;, that is, they
must implements fit and transform methods.
The final estimator needs only implements fit.</p>
<p>The purpose of the pipeline is to assemble several steps that can be
cross-validated together while setting different parameters.
For this, it enables setting parameters of the various steps using their
names and the parameter name separated by a &#8216;__&#8217;, as in the example below.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>steps: list</dt>
<dd>List of (name, transform) tuples (implementing fit/transform) that are
chained, in the order in which they are chained, with the last object
an estimator.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">samples_generator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># generate some data to play with</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">samples_generator</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># ANOVA SVM-C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_filter</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_regression</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;anova&#39;</span><span class="p">,</span> <span class="n">anova_filter</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="p">)])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># You can set the parameters using the names issued</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># For instance, fit using a k of 10 in the SelectKBest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># and a parameter &#39;C&#39; of the svn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">anova__k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">svc__C</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>                                             
<span class="go">Pipeline(steps=[...])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="n">anova_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.75</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PipelineSklearn</strong></li>
<li><strong>PipelineSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ProbabilisticPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Additional layer on top of PCA that adds a probabilistic evaluationPrincipal component analysis (PCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.ProbabilisticPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data and keeping only the most significant singular vectors to project the
data to a lower dimensional space.</p>
<p>This implementation uses the scipy.linalg implementation of the singular
value decomposition. It only works for dense arrays and is not scalable to
large dimensional data.</p>
<p>The time complexity of this implementation is <tt class="docutils literal"><span class="pre">O(n</span> <span class="pre">**</span> <span class="pre">3)</span></tt> assuming
n ~ n_samples ~ n_features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, None or string</span></dt>
<dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">if n_components == &#8216;mle&#8217;, Minka&#8217;s MLE is used to guess the dimension
if <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></tt>, select the number of components such that
the amount of variance that needs to be explained is greater than the
percentage specified by n_components</p>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by n_samples times singular values to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making there data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For n_components=&#8217;mle&#8217;, this class uses the method of <a href="#id24"><span class="problematic" id="id25">`</span></a>Thomas P. Minka:</p>
<p>Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`</p>
<p>Due to implementation subtleties of the Singular Value Decomposition (SVD),
which is used in this implementation, running fit twice on the same matrix
can lead to principal components with signs flipped (change in direction).
For this reason, it is important to always use the same estimator object to
transform data in a consistent fashion.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>ProbabilisticPCA
RandomizedPCA
KernelPCA
SparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ProbabilisticPCASklearnNode</strong></li>
<li><strong>ProbabilisticPCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ProjectedGradientNMFSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Non-Negative matrix factorization by Projected Gradient (NMF)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.nmf.ProjectedGradientNMF</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int or None</dt>
<dd>Number of components, if n_components is not set all components
are kept</dd>
<dt>init:  &#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;random&#8217;</dt>
<dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<div class="last highlight-python"><pre>'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
    initialization (better for sparseness)
'nndsvda': NNDSVD with zeros filled with the average of X
    (better when sparsity is not desired)
'nndsvdar': NNDSVD with zeros filled with small random values
    (generally faster, less accurate alternative to NNDSVDa
    for when sparsity is not desired)
'random': non-negative random matrices</pre>
</div>
</dd>
<dt>sparseness: &#8216;data&#8217; | &#8216;components&#8217; | None, default: None</dt>
<dd>Where to enforce sparsity in the model.</dd>
<dt>beta: double, default: 1</dt>
<dd>Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness.</dd>
<dt>eta: double, default: 0.1</dt>
<dd>Degree of correctness to mantain, if sparsity is not None. Smaller
values mean larger error.</dd>
<dt>tol: double, default: 1e-4</dt>
<dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter: int, default: 200</dt>
<dd>Number of iterations to compute.</dd>
<dt>nls_max_iter: int, default: 2000</dt>
<dd>Number of iterations in NLS subproblem.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Random number generator seed control.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Non-negative components of the data</dd>
<dt><cite>reconstruction_err_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">number</span></dt>
<dd>Frobenius norm of the matrix difference between the
training data and the reconstructed data from the
fit produced by the model. <tt class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></tt>
Not computed for sparse input matrices because it is
too expensive in terms of memory.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">ProjectedGradientNMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">        n_components=2, nls_max_iter=2000, random_state=0, sparseness=None,</span>
<span class="go">        tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 0.77032744,  0.11118662],</span>
<span class="go">       [ 0.38526873,  0.38228063]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00746...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">sparseness</span><span class="o">=</span><span class="s">&#39;components&#39;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">            n_components=2, nls_max_iter=2000, random_state=0,</span>
<span class="go">            sparseness=&#39;components&#39;, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 1.67481991,  0.29614922],</span>
<span class="go">       [-0.        ,  0.4681982 ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.513...</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>This implements</p>
<p>C.-J. Lin. Projected gradient methods
for non-negative matrix factorization. Neural
Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>P. Hoyer. Non-negative Matrix Factorization with
Sparseness Constraints. Journal of Machine Learning
Research 2004.</p>
<p>NNDSVD is introduced in</p>
<p>C. Boutsidis, E. Gallopoulos: SVD based
initialization: A head start for nonnegative
matrix factorization - Pattern Recognition, 2008
<a class="reference external" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ProjectedGradientNMFSklearnNode</strong></li>
<li><strong>ProjectedGradientNMFSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-qdasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.QDASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.QDASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.QDASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-qdasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.QDASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">QDASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.QDASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Quadratic Discriminant Analysis (QDA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.qda.QDA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A classifier with a quadratic decision boundary, generated
by fitting class conditional densities to the data
and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span></dt>
<dd>Priors on classes</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>covariances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of array-like, shape = [n_features, n_features]</span></dt>
<dd>Covariance matrices of each class.</dd>
<dt><cite>means_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Class means.</dd>
<dt><cite>priors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><cite>rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>For each class an array of shape [n_samples, n_samples], the
rotation of the Gaussian distribution, i.e. its principal axis.</dd>
<dt><cite>scalings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Contains the scaling of the Gaussian
distributions along the principal axes for each
class, i.e. the variance in the rotated coordinate system.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.qda</span> <span class="kn">import</span> <span class="n">QDA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">QDA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">QDA(priors=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.lda.LDA: Linear discriminant analysis</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>QDASklearn</strong></li>
<li><strong>QDASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RFECVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature ranking with recursive feature elimination and cross-validated
selection of the best number of features.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFECV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int or cross-validation generator, optional (default=None)</span></dt>
<dd>If int, it is the number of folds.
If None, 3-fold cross-validation is performed by default.
Specific cross-validation objects can also be passed, see
<cite>sklearn.cross_validation module</cite> for details.</dd>
<dt>loss_function <span class="classifier-delimiter">:</span> <span class="classifier">function, optional (default=None)</span></dt>
<dd>The loss function to minimize by cross-validation. If None, then the
score function of the estimator is maximized.</dd>
<dt>estimator_params <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameters for the external estimator.
Useful for doing grid searches.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default=0</span></dt>
<dd>Controls verbosity of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>n_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features with cross-validation.</dd>
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><cite>ranking_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <cite>ranking_[i]</cite>
corresponds to the ranking
position of the i-th feature.
Selected (i.e., estimated best)
features are assigned rank 1.</dd>
<dt><cite>cv_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_subsets_of_features]</span></dt>
<dd>The cross-validation scores such that
<cite>cv_scores_[i]</cite> corresponds to
the CV score of the i-th subset of features.</dd>
<dt><cite>estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the a-priori not known 5
informative features in the Friedman #1 dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFECV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RFECVSklearn</strong></li>
<li><strong>RFECVSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-rfesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RFESklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RFESklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFESklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-rfesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RFESklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RFESklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RFESklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature ranking with recursive feature elimination.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFE</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination
(RFE) is to select features by recursively considering smaller and smaller
sets of features. First, the estimator is trained on the initial set of
features and weights are assigned to each one of them. Then, features whose
absolute weights are the smallest are pruned from the current set features.
That procedure is recursively repeated on the pruned set until the desired
number of features to select is eventually reached.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>n_features_to_select <span class="classifier-delimiter">:</span> <span class="classifier">int or None (default=None)</span></dt>
<dd>The number of features to select. If <cite>None</cite>, half of the features
are selected.</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>estimator_params <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameters for the external estimator.
Useful for doing grid searches.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>n_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features.</dd>
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><cite>ranking_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <cite>ranking_[i]</cite> corresponds to the         ranking position of the i-th feature. Selected (i.e., estimated         best) features are assigned rank 1.</dd>
<dt><cite>estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the 5 right informative
features in the Friedman #1 dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RFESklearn</strong></li>
<li><strong>RFESklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RadiusNeighborsClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier implementing a vote among neighbors within a given radius</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.classification.RadiusNeighborsClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span></dt>
<dd>Range of parameter space to use by default for :meth`radius_neighbors`
queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">BallTree</span></tt></li>
<li>&#8216;kd_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">scipy.spatial.cKDtree</span></tt></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <tt class="xref py py-meth docutils literal"><span class="pre">fit()</span></tt> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or cKDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>p: integer, optional (default = 2)</dt>
<dd>Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>outlier_label: int, optional (default = None)</dt>
<dd>Label, which is given for outlier samples (samples with no
neighbors on given radius).
If set to None, ValueError is raised, when outlier is detected.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsClassifier</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0]</span>
</pre></div>
</div>
<p>See also</p>
<p>KNeighborsClassifier
RadiusNeighborsRegressor
KNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <em class="xref std std-ref">Nearest Neighbors</em> in the online documentation
for a discussion of the choice of <tt class="docutils literal"><span class="pre">algorithm</span></tt> and <tt class="docutils literal"><span class="pre">leaf_size</span></tt>.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RadiusNeighborsClassifierSklearnNode</strong></li>
<li><strong>RadiusNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomForestClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A random forest classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A random forest is a meta estimator that fits a number of classifical
decision trees on various sub-samples of the dataset and use averaging
to improve the predictive accuracy and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on</li>
</ul>
</li>
<li>classification tasks and <cite>max_features=n_features</cite> on regression</li>
<li>problems.</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel. If -1, then the number of jobs
is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>classes_</cite>: array of shape = [n_classes] or a list of such arrays</dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite>: int or list</dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_decision_function_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeClassifier, ExtraTreesClassifier</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomForestClassifierSklearn</strong></li>
<li><strong>RandomForestClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomForestRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A random forest regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A random forest is a meta estimator that fits a number of classifical
decision trees on various sub-samples of the dataset and use averaging
to improve the predictive accuracy and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite> on</li>
</ul>
</li>
<li>classification tasks and <cite>max_features=n_features</cite></li>
<li>on regression problems.</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>compute_importances <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether feature importances are computed and stored into the
<tt class="docutils literal"><span class="pre">feature_importances_</span></tt> attribute when calling fit.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel. If -1, then the number of jobs
is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature mportances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_prediction_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeRegressor, ExtraTreesRegressor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomForestRegressorSklearn</strong></li>
<li><strong>RandomForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomTreesEmbeddingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An ensemble of totally random trees.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomTreesEmbedding</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as trees in the forest.</p>
<p>The dimensionality of the resulting representation is approximately
<tt class="docutils literal"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></tt>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of trees in the forest.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum depth of each tree.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_density <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>This parameter controls a trade-off in an optimization heuristic. It
controls the minimum density of the <cite>sample_mask</cite> (i.e. the
fraction of samples in the mask). If the density falls below this
threshold the mask is recomputed and the input data is packed
which results in data copying.  If <cite>min_density</cite> equals to one,
the partitions are always represented as copies of the original
data. Otherwise, partitions are represented as bit masks (aka
sample masks).</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel. If -1, then the number of jobs
is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Moosmann, F. and Triggs, B. and Jurie, F.  &#8220;Fast discriminative
visual codebooks using randomized clustering forests&#8221;
NIPS 2007</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomTreesEmbeddingSklearnNode</strong></li>
<li><strong>RandomTreesEmbeddingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedLassoSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Randomized Lasso</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLasso</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Randomized Lasso works by resampling the train data and computing
a Lasso on each resampling. In short, the features selected more
often are good features. It is also known as stability selection.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, &#8216;aic&#8217;, or &#8216;bic&#8217;</span></dt>
<dd>The regularization parameter alpha parameter in the Lasso.
Warning: this is not the alpha parameter in the stability selection
article which is scaling.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the regressors X are normalized</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217;</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to &#8216;auto&#8217; let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform in the Lars algorithm.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the &#8216;tol&#8217; parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediatly
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is thepath to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><cite>all_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <tt class="docutils literal"><span class="pre">scores_</span></tt> is the max of         <tt class="docutils literal"><span class="pre">all_scores_</span></tt>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLasso</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_lasso</span> <span class="o">=</span> <span class="n">RandomizedLasso</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_sparse_recovery.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLogisticRegression, LogisticRegression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedLassoSklearn</strong></li>
<li><strong>RandomizedLassoSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedLogisticRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Randomized Logistic Regression</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLogisticRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Randomized Regression works by resampling the train data and computing
a LogisticRegression on each resampling. In short, the features selected
more often are good features. It is also known as stability selection.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The regularization parameter C in the LogisticRegression.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the regressors X are normalized</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>tolerance for stopping criteria of LogisticRegression</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediatly
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is thepath to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><cite>all_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <tt class="docutils literal"><span class="pre">scores_</span></tt> is the max         of <tt class="docutils literal"><span class="pre">all_scores_</span></tt>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_logistic</span> <span class="o">=</span> <span class="n">RandomizedLogisticRegression</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_randomized_lasso.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLasso, Lasso, ElasticNet</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedLogisticRegressionSklearnNode</strong></li>
<li><strong>RandomizedLogisticRegressionSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Principal component analysis (PCA) using randomized SVD</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.RandomizedPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using approximated Singular Value
Decomposition of the data and keeping only the most significant
singular vectors to project the data to a lower dimensional space.</p>
<p>This implementation uses a randomized SVD implementation and can
handle both scipy.sparse and numpy dense arrays as input.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum number of components to keep: default is 50.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten</dd>
<dt>iterated_power <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of iteration for the power method. 3 by default.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by the singular values to ensure uncorrelated outputs with unit
component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState instance or None (default)</span></dt>
<dd>Pseudo Random Number generator seed control. If None, use the
numpy.random singleton.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                 
<span class="go">RandomizedPCA(copy=True, iterated_power=3, n_components=2,</span>
<span class="go">       random_state=None, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
ProbabilisticPCA</p>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="halko2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Halko2009]</td><td><cite>Finding structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions Halko, et al., 2009
(arXiv:909)</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mrt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MRT]</td><td><cite>A randomized algorithm for the decomposition of matrices
Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedPCASklearnNode</strong></li>
<li><strong>RandomizedPCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RidgeClassifierCVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Ridge classifier with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifierCV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation. Currently, only the n_features &gt;
n_samples case is handled efficiently.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas: numpy array of shape [n_alphas]</dt>
<dd>Array of alpha values to try.
Small positive values of alpha improve the conditioning of the
problem and reduce the variance of the estimates.
Alpha corresponds to (2*C)^-1 in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the regressors X are normalized</dd>
<dt>score_func: callable, optional</dt>
<dd>function that takes 2 arguments and compares them in
order to evaluate the performance of prediction (big is good)
if None is passed, the score of the estimator is maximized</dd>
<dt>loss_func: callable, optional</dt>
<dd>function that takes 2 arguments and compares them in
order to evaluate the performance of prediction (small is good)
if None is passed, the score of the estimator is maximized</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">cross-validation generator, optional</span></dt>
<dd>If None, Generalized Cross-Validation (efficient Leave-One-Out)
will be used.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Weights associated with classes in the form
{class_label : weight}. If not given, all classes are
supposed to have weight one.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>cv_values_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional</span></dt>
<dd>Cross-validation values for each alpha (if <cite>store_cv_values=True</cite> and</dd>
</dl>
<p><cite>cv=None</cite>). After <cite>fit()</cite> has been called, this attribute will contain     the mean squared errors (by default) or the values of the     <cite>{loss,score}_func</cite> function (if provided in the constructor).</p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span></dt>
<dd>Weight vector(s).</dd>
<dt><cite>alpha_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimated regularization parameter</dd>
</dl>
<p>See also</p>
<p>Ridge: Ridge regression
RidgeClassifier: Ridge classifier
RidgeCV: Ridge regression with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RidgeClassifierCVSklearnNode</strong></li>
<li><strong>RidgeClassifierCVSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RidgeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier using Ridge regression.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<tt class="docutils literal"><span class="pre">(2*C)^-1</span></tt> in other linear models such as LogisticRegression or
LinearSVC.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Weights associated with classes in the form
{class_label : weight}. If not given, all classes are
supposed to have weight one.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (e.g. data is expected to be
already centered).</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations for conjugate gradient solver.
The default value is determined by scipy.sparse.linalg.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the regressors X are normalized</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;dense_cholesky&#8217;, &#8216;lsqr&#8217;, &#8216;sparse_cg&#8217;}</span></dt>
<dd>Solver to use in the computational
routines. &#8216;dense_cholesky&#8217; will use the standard
scipy.linalg.solve function, &#8216;sparse_cg&#8217; will use the
conjugate gradient solver as found in
scipy.sparse.linalg.cg while &#8216;auto&#8217; will chose the most
appropriate depending on the matrix X. &#8216;lsqr&#8217; uses
a direct regularized least-squares routine provided by scipy.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Precision of the solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_classes, n_features]</span></dt>
<dd>Weight vector(s).</dd>
</dl>
<p>See also</p>
<p>Ridge, RidgeClassifierCV</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RidgeClassifierSklearn</strong></li>
<li><strong>RidgeClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SGDClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear model fitted by minimizing a regularized empirical loss with SGD.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense or sparse arrays
of floating point values for the features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;hinge&#8217; or &#8216;log&#8217; or &#8216;modified_huber&#8217;</span></dt>
<dd>The loss function to be used. Defaults to &#8216;hinge&#8217;. The hinge loss is
a margin loss used by standard linear SVM models. The &#8216;log&#8217; loss is
the loss of logistic regression models and can be used for
probability estimation in binary classifiers. &#8216;modified_huber&#8217;
is another smooth loss that brings tolerance to outliers.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; migh bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>epsilon: float</dt>
<dd>Epsilon in the epsilon-insensitive loss functions;
only if <cite>loss==&#8217;huber&#8217;</cite> or <cite>loss=&#8217;epsilon_insensitive&#8217;</cite>.
If the difference between the current prediction and the correct label
is below this threshold, the model is not updated.</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0/(t+t0) [default]</li>
<li>invscaling: eta = eta0 / pow(t, power_t)</li>
</ul>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The initial learning rate [default 0.01].</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.5].</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label</span> <span class="classifier-delimiter">:</span> <span class="classifier">weight} or &#8220;auto&#8221; or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;auto&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies.</p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,</span>
<span class="go">        fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;,</span>
<span class="go">        loss=&#39;hinge&#39;, n_iter=5, n_jobs=1, penalty=&#39;l2&#39;, power_t=0.5,</span>
<span class="go">        random_state=None, rho=None, shuffle=False,</span>
<span class="go">        verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>LinearSVC, LogisticRegression, Perceptron</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SGDClassifierSklearnNode</strong></li>
<li><strong>SGDClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SGDRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear model fitted by minimizing a regularized empirical loss with SGD</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;squared_loss&#8217; or &#8216;huber&#8217;</span></dt>
<dd>The loss function to be used. Defaults to &#8216;squared_loss&#8217; which refers
to the ordinary least squares fit. &#8216;huber&#8217; is an epsilon insensitive
loss function for robust regression.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; migh bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level.</dd>
<dt>epsilon: float</dt>
<dd>Epsilon in the epsilon-insensitive loss functions;
only if <cite>loss==&#8217;huber&#8217;</cite> or <cite>loss=&#8217;epsilon_insensitive&#8217;</cite>.
If the difference between the current prediction and the correct label
is below this threshold, the model is not updated.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0/(t+t0)</li>
<li>invscaling: eta = eta0 / pow(t, power_t) [default]</li>
</ul>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span></dt>
<dd>The initial learning rate [default 0.01].</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.25].</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Weights asigned to the features.</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span></dt>
<dd>The intercept term.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDRegressor(alpha=0.0001, epsilon=0.1, eta0=0.01, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;,</span>
<span class="go">       n_iter=5, p=None, penalty=&#39;l2&#39;, power_t=0.25, random_state=None,</span>
<span class="go">       rho=None, shuffle=False, verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge, ElasticNet, Lasso, SVR</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SGDRegressorSklearn</strong></li>
<li><strong>SGDRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-svcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-svcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>C-Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.svm.classes.SVC</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The implementations is a based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to dataset with more than a couple of 10000 samples.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <cite>gamma</cite>, <cite>coef0</cite> and <cite>degree</cite> affect each,
see the corresponding section in the narrative documentation:</p>
<p><em class="xref std std-ref">svm_kernels</em>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of kernel function.
It is significant only in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Kernel coefficient for &#8216;rbf&#8217; and &#8216;poly&#8217;.
If gamma is 0.0 then 1/n_features will be used instead.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>probability: boolean, optional (default=False)</dt>
<dd>Whether to enable probability estimates. This must be enabled prior
to calling predict_proba.</dd>
<dt>shrinking: boolean, optional (default=True)</dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB)</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The &#8216;auto&#8217; mode uses the values of y to
automatically adjust weights inversely proportional to
class frequencies.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Index of support vectors.</dd>
<dt><cite>support_vectors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><cite>n_support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
<dd>number of support vector for each class.</dd>
<dt><cite>dual_coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.         For multiclass, coefficient for all 1-vs-1 classifiers.         The layout of the coefficients in the multiclass case is somewhat         non-trivial. See the section about multi-class classification in the         SVM section of the User Guide for details.</dd>
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_features]</span></dt>
<dd><p class="first">Weights asigned to the features (coefficients in the primal
problem). This is only available in the case of linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">        gamma=0.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">        shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVR</dt>
<dd>Support Vector Machine for Regression implemented using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable Linear Support Vector Machine for classififcation
implemented using liblinear. Check the See also section of
LinearSVC for more comparison element.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SVCSklearnNode</strong></li>
<li><strong>SVCSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-scalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-scalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ScalerSklearnNode</strong></li>
<li><strong>ScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFdrSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the p-values for an estimated false discovery rate</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFdr</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This uses the Benjamini-Hochberg procedure. <tt class="docutils literal"><span class="pre">alpha</span></tt> is the target false
discovery rate.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFdrSklearn</strong></li>
<li><strong>SelectFdrSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFprSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the pvalues below alpha based on a FPR test.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFpr</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>FPR test stands for False Positive Rate test. It controls the total
amount of false detections.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest p-value for features to be kept.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFprSklearnNode</strong></li>
<li><strong>SelectFprSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFweSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the p-values corresponding to Family-wise error rate</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFwe</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFweSklearnNode</strong></li>
<li><strong>SelectFweSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectKBestSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Select features according to the k highest scores.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectKBest</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>k <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=10</span></dt>
<dd>Number of top features to select.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectKBestSklearn</strong></li>
<li><strong>SelectKBestSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectPercentileSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Select features according to a percentile of the highest scores.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectPercentile</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>percentile <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=10</span></dt>
<dd>Percent of features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal p-values will be broken in an unspecified
way.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectPercentileSklearn</strong></li>
<li><strong>SelectPercentileSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SparseCoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Sparse coding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.SparseCoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a sparse representation of data against a fixed, precomputed
dictionary.</p>
<p>Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array <cite>code</cite> such that:</p>
<div class="highlight-python"><pre>X ~= code * dictionary</pre>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dictionary <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The dictionary atoms used for sparse coding. Lines are assumed to be
normalized to unit norm.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd><p class="first">Algorithm used to transform the data:</p>
<ul class="last simple">
<li>lars: uses the least angle regression method (linear_model.lars_path)</li>
<li>lasso_lars: uses Lars to compute the Lasso solution</li>
<li>lasso_cd: uses the coordinate descent method to compute the</li>
<li>Lasso solution (linear_model.Lasso). lasso_lars will be faster if</li>
<li>the estimated components are sparse.</li>
<li>omp: uses orthogonal matching pursuit to estimate the sparse solution</li>
<li>threshold: squashes to zero all coefficients less than alpha from</li>
<li>the projection <tt class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></tt></li>
</ul>
</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The unchanged dictionary atoms</dd>
</dl>
<p>See also</p>
<p>DictionaryLearning
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA
sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SparseCoderSklearnNode</strong></li>
<li><strong>SparseCoderSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SparsePCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Sparse Principal Components Analysis (SparsePCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.SparsePCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Number of sparse atoms to extract.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Tolerance for the stopping condition.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Number of parallel jobs to run.</dd>
<dt>U_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>Initial values for the loadings for warm restart scenarios.</dd>
<dt>V_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>Initial values for the components for warm restart scenarios.</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>Degree of verbosity of the printed output.</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Vector of errors at each iteration.</dd>
</dl>
<p>See also</p>
<p>PCA
MiniBatchSparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SparsePCASklearnNode</strong></li>
<li><strong>SparsePCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">StandardScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Standardize features by removing the mean and scaling to unit variance</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Centering and scaling happen indepently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using the
<cite>transform</cite> method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual feature do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).</p>
<p>For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>with_mean <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, center the data before scaling.</dd>
<dt>with_std <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>Set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix and if axis is 1).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>mean_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span></dt>
<dd>The mean value for each feature in the training set.</dd>
<dt><cite>std_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span></dt>
<dd>The standard deviation for each feature in the training set.</dd>
</dl>
<p>See also</p>
<p><tt class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.scale()</span></tt> to perform centering and
scaling without using the <tt class="docutils literal"><span class="pre">Transformer</span></tt> object oriented API</p>
<p><tt class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.RandomizedPCA</span></tt> with <cite>whiten=True</cite>
to further remove the linear correlation across features.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>StandardScalerSklearnNode</strong></li>
<li><strong>StandardScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">TfidfTransformerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Transform a count matrix to a normalized tf or tf–idf representation</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfTransformer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Tf means term-frequency while tf–idf means term-frequency times inverse
document-frequency. This is a common term weighting scheme in information
retrieval, that has also found good use in document classification.</p>
<p>The goal of using tf–idf instead of the raw frequencies of occurrence of a
token in a given document is to scale down the impact of tokens that occur
very frequently in a given corpus and that are hence empirically less
informative than features that occur in a small fraction of the training
corpus.</p>
<p>In the SMART notation used in IR, this class implements several tf–idf
variants. Tf is always &#8220;n&#8221; (natural), idf is &#8220;t&#8221; iff use_idf is given,
&#8220;n&#8221; otherwise, and normalization is &#8220;c&#8221; iff norm=&#8217;l2&#8217;, &#8220;n&#8221; iff norm=None.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="yates2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Yates2011]</td><td><cite>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
Information Retrieval. Addison Wesley, pp. 68–74.</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="msr2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MSR2008]</td><td><cite>C.D. Manning, H. Schütze and P. Raghavan (2008). Introduction
to Information Retrieval. Cambridge University Press,
pp. 121–125.</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>TfidfTransformerSklearnNode</strong></li>
<li><strong>TfidfTransformerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">TfidfVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of raw documents to a matrix of TF-IDF features.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Equivalent to CountVectorizer followed by TfidfTransformer.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span></dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>charset <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span></dt>
<dd>If bytes or files are given to analyze, this charset is used to
decode.</dd>
<dt>charset_error <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>charset</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span></dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span></dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned is currently the only
supported string value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Convert all characters to lowercase befor tokenizing.</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1.0 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly higher than the given threshold (corpus specific stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 2 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly lower than the given threshold.
This value is also called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">optional, None by default</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, False by default.</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>CountVectorizer</dt>
<dd>Tokenize the documents and count the occurrences of token and return
them as a sparse matrix</dd>
<dt>TfidfTransformer</dt>
<dd>Apply Term Frequency Inverse Document Frequency normalization to a
sparse matrix of occurrence counts.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>TfidfVectorizerSklearnNode</strong></li>
<li><strong>TfidfVectorizerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">WardAgglomerationSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature agglomeration based on Ward hierarchical clustering</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.cluster.hierarchical.WardAgglomeration</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int or ndarray</span></dt>
<dd>The number of clusters.</dd>
<dt>connectivity <span class="classifier-delimiter">:</span> <span class="classifier">sparse matrix</span></dt>
<dd>connectivity matrix. Defines for each feature the neigbhoring
features following a given structure of the data.
Default is None, i.e, the hiearchical agglomeration algorithm is
unstructured.</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span></dt>
<dd>Used to cache the output of the computation of the tree.
By default, no caching is done. If a string is given, it is the
path to the caching directory.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Copy the connectivity matrix or work inplace.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int (optional)</span></dt>
<dd>The number of connected components in the graph defined by the
connectivity matrix. If not set, it is estimated.</dd>
<dt>compute_full_tree: bool or &#8216;auto&#8217; (optional)</dt>
<dd>Stop early the construction of the tree at n_clusters. This is
useful to decrease computation time if the number of clusters is
not small compared to the number of samples. This option is
useful only when specifying a connectivity matrix. Note also that
when varying the number of cluster and using caching, it may
be advantageous to compute the full tree.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>children_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_nodes, 2]</span></dt>
<dd>List of the children of each nodes.
Leaves of the tree do not appear.</dd>
<dt><cite>labels_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array [n_samples]</span></dt>
<dd>cluster labels for each point</dd>
<dt><cite>n_leaves_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of leaves in the hiearchical tree.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">Possible NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>WardAgglomerationSklearn</strong></li>
<li><strong>WardAgglomerationSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.socket_utils.html" title="socket_utils"
             >previous</a> |</li>
        <li><a href="index.html">pySPACE 0.5 alpha documentation</a> &raquo;</li>
          <li><a href="content.html" >Table of Contents</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, pySPACE Developer Team.
      Last updated on Aug 07, 2013.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>